#!/usr/bin/env python
# coding: utf-8

# ## Introduction
# In this project, a machine learning classification model is developed using Lidar point cloud data obtained from the UK Defra portal to predict the classification of Lidar points for London, employing the XGBoost classification algorithm.
# 
# #### Lidar Point Cloud
# LiDAR point clouds are of paramount importance in various fields due to their ability to provide highly accurate and detailed three-dimensional representations of the Earth's surface. These datasets are instrumental in applications such as urban planning, environmental monitoring, forestry management, infrastructure assessment, and autonomous vehicle navigation. LiDAR point clouds enable precise spatial analysis, aiding decision-making processes and contributing to advancements in fields that rely on accurate geospatial information.
# 
# #### Lidar Point Cloud Classification
# The classification of LiDAR point clouds is crucial for extracting meaningful information from the vast datasets generated by LiDAR sensors. By categorizing points into classes such as ground, vegetation, buildings, and other objects, it enables accurate terrain modelling, vegetation analysis, and urban planning. This classification is essential for applications like flood modelling, land cover mapping, and infrastructure management, enhancing the usability of LiDAR data for diverse industries and decision-making processes.

# ### Import Libraries

# In[1]:


import laspy                                                               # For reading and writing LAS (LIDAR) files
import numpy as np                                                         # For statstical analysis
import pandas as pd                                                        # For data manupulation
from sklearn.model_selection import train_test_split                       # For splitting datasets into train & test sets
from imblearn.over_sampling import RandomOverSampler                       # For balancing the class distribution
import xgboost as xgb                                                      # For ML model building
from sklearn.metrics import accuracy_score, classification_report          # For evaluation
from sklearn.preprocessing import LabelEncoder                             # For numeric categorical Encoding 
import pickle                                                              # For seriealizing.


# In[2]:


import warnings
warnings.filterwarnings('ignore')


# ### Download the dataset

# Read the LAS file using laspy 2.0
las_file = laspy.read('TQ5678_P_11403_20180417_20180417.laz')

# Access the LAS points and their attributes
x = las_file['X']
y = las_file['Y']
z = las_file['Z']
intensity = las_file['intensity']

# Print the first 10 points
for i in range(10):
    print(f"Point {i + 1}: X={x[i]}, Y={y[i]}, Z={z[i]}, Intensity={intensity[i]}")


# ### Data Information

# In[4]:


las_file.header.point_count # Total number of cloud points


# In[5]:


list(las_file.point_format.dimension_names) # Available features for the LiDAR file


# In[6]:


print('X-coordinate : ',las_file.x)
print('Y-coordinate : ',las_file.y)
print('Z-coordinate : ',las_file.z)
print('Intensity : ',las_file.intensity)
print('Return numbers : ',las_file.return_number)
print('Number of Returns : ',las_file.number_of_returns)
print('Scan Direction Flags : ', las_file.scan_direction_flag)
print('Edge of flight line : ', las_file.edge_of_flight_line)
print('Classification : ', las_file.classification)
print('Synthetic : ', las_file.synthetic)
print('Key point : ', las_file.key_point)
print('Withheld : ',las_file.withheld)
print('Scan angle rank : ', las_file.scan_angle_rank)
print('User data : ' , las_file.user_data)
print('Point source id : ', las_file.point_source_id)
print('GPS time : ', las_file.gps_time)


# In[ ]:





# In[7]:


set(list(las_file.return_number))


# In[8]:


set(list(las_file.number_of_returns))


# In[9]:


set(list(las_file.scan_direction_flag))


# In[10]:


set(list(las_file.edge_of_flight_line))


# In[11]:


set(list(las_file.synthetic))


# In[12]:


set(list(las_file.key_point))


# In[13]:


set(list(las_file.withheld))


# In[14]:


set(list(las_file.scan_angle_rank))


# In[15]:


set(list(las_file.user_data))


# In[16]:


set(list(las_file.point_source_id))


# In[17]:


set(list(las_file.classification))


# In our LiDAR point cloud classification task, the dataset comprises six distinct classes. To better understand these classes, we referred to the ArcGIS documentation(https://pro.arcgis.com/en/pro-app/latest/tool-reference/3d-analyst/change-las-class-codes.htm)  for comprehensive details. Here is a brief overview of each class:
# 
# #### Class 0: Unclassified
# 
# This class is created but not classified.
# 
# #### Class 1: Unassigned
# 
# This class is designated for points that have not been assigned to any specific category.
# 
# #### Class 2: Ground
# 
# Points belonging to the ground surface are categorized under this class. Ground points play a crucial role in terrain modeling and elevation analysis.
# 
# #### Class 3: Low Vegetation
# 
# Points corresponding to low-lying vegetation, such as grass or shrubs, are classified as low vegetation. This class contributes valuable information for vegetation analysis.
# 
# #### Class 4: Medium Vegetation
# 
# Points representing medium-height vegetation, which may include bushes and small trees, fall into this category. Identifying medium vegetation is essential for land cover and vegetation studies.
# 
# #### Class 5: High Vegetation
# 
# Tall vegetation, such as mature trees, is categorized as high vegetation. This class is significant for detailed analysis of forested areas and canopy structure.
# 
# #### Class 6: Building
# 
# Class 6 typically represents buildings in the Lidar point cloud
# 
# #### Class 7: Low Point (Noise)
# 
# Points that are considered noise or outliers in the data.These points may not represent the actual terrain or objects of interest.

# In[18]:


# Check if the data is balance or not
# Extract class labels
class_labels = las_file.raw_classification

# Convert class labels to a Pandas DataFrame for analysis
df = pd.DataFrame({'class_label': class_labels})

# Use value_counts to get class counts
class_counts = df['class_label'].value_counts()

# Display class counts
print(class_counts)


# Data is not balanced

# In[ ]:





# ### Split data into Input and Output variables


# In[48]:


X = np.vstack([las_file.x, las_file.y, las_file.z, las_file.intensity,
               las_file.return_number,las_file.number_of_returns,las_file.scan_direction_flag,
              las_file.edge_of_flight_line,las_file.scan_angle_rank,las_file.user_data,
              las_file.point_source_id,las_file.gps_time]).transpose()


# In[49]:

y = np.where(las_file.classification == 1, 2,
             np.where(las_file.classification == 2, 3,
                      np.where(las_file.classification == 3, 4,
                               np.where(las_file.classification == 4, 5,
                                           np.where(las_file.classification == 5, 6, 
                                                np.where(las_file.classification == 6, 7, 0))))))


# In[50]:


unique_values = np.unique(y)
print("Unique values in y_train:", unique_values)


# In[51]:


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)


# In[ ]:





# ### Balance the train data

# In[52]:


# Create the oversampler
oversampler = RandomOverSampler(sampling_strategy='auto', random_state=42)
# Fit and apply the oversampler to the training data
X_resampled, y_resampled = oversampler.fit_resample(X_train, y_train)


# In[53]:


# Use numpy.unique to get unique class labels
unique_classes = np.unique(y_resampled)

# Use numpy.bincount to count occurrences of each class label
class_counts = np.bincount(y_resampled)

# Display class counts
for class_label, count in zip(unique_classes, class_counts[unique_classes]):
    print(f'Class {class_label}: {count} points')


# Data is balanced now.

# ### Train the Machine Learning Model

# In[54]:


# Create a label encoder
label_encoder = LabelEncoder()

# Fit and transform the label encoder on y_train
y_train_encoded = label_encoder.fit_transform(y_resampled)


# In[ ]:





# ### XGBoost Classifier

# In[55]:


# Create and train an XGBoost model
model = xgb.XGBClassifier(objective="multi:softmax", num_class=9)
model.fit(X_resampled, y_train_encoded)

# Make predictions
y_pred_encoded = model.predict(X_test)

# Decode the predictions back to original class labels
y_pred = label_encoder.inverse_transform(y_pred_encoded)


# ### Model Evaluation

# ### XGBoost classifier

# In[56]:


# Evaluate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Validation Accuracy:", accuracy)


# In[57]:


# Additional metrics
print("Classification Report:")
print(classification_report(y_test, y_pred))


# ### Make Prediction

# In[58]:


new_data_point = np.array([[526000.52, 81826.25, 29.37, 78, 1, 1, 1, 1,6,6,5,1.20805091e+09]])

# Make the prediction
prediction = model.predict(new_data_point)

# Display the prediction
print("Predicted class:", prediction)


# In[ ]:





# ### Save the model

# In[59]:


pickle.dump(model, open('model.pkl','wb'))
model = pickle.load(open('model.pkl','rb'))


# In[38]:


print(model.predict([[526000.52, 81826.25, 29.37, 78, 1, 1, 1, 1,6,6,5,1.20805091e+09]]))


# In[ ]:




